\documentclass[a4paper, 11pt]{article}

% Внешние пакеты
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[main = russian, english]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{indentfirst}

\usepackage{nicefrac}                % Красивые дроби
\usepackage{amsthm}                  % Красивый внешний вид теорем, определений и доказательств
% \usepackage[integrals]{wasysym}    % Делает интегралы прямыми, но некрасивыми
% \usepackage{bigints}               % Позволяет делать большущие интегралы

% Красивый внешний вид теорем, определений и доказательств
\newtheoremstyle{def}
        {\topsep}
        {\topsep}
        {\normalfont}
        {\parindent}
        {\bfseries}
        {.}
        {.5em}
        {}
\theoremstyle{def}
\newtheorem{definition}{Определение}[section]

\newtheoremstyle{th}
        {\topsep}
        {\topsep}
        {\itshape}
        {\parindent}
        {\bfseries}
        {.}
        {.5em}
        {}
\theoremstyle{th}
\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}
\newtheorem{assertion}{Утверждение}[section]

\newtheoremstyle{rem}
        {0.5\topsep}
        {0.5\topsep}
        {\normalfont}
        {\parindent}
        {\itshape}
        {.}
        {.5em}
        {}
\theoremstyle{rem}
\newtheorem{remark}{Замечание}[section]

% Новое доказательство
\renewenvironment{proof}{\parД о к а з а т е л ь с т в о.}{\hfill$\blacksquare$}

% Нумерация рисунков
\usepackage{chngcntr}
\counterwithin{figure}{section}

% Переопределение математических штук
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\mbox{ar}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\X}{\mathcal{X}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\const}{const}

\begin{document}
        \thispagestyle{empty}
\begin{center}
    \ \vspace{-3cm}

    \includegraphics[width=0.5\textwidth]{msu.eps}\\

    {\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
    Факультет вычислительной математики и кибернетики\\
    Кафедра системного анализа

    \vfill

    {\LARGE Отчет по практикуму}

    \vspace{1cm}

    {\Huge\bfseries <<Стохастический анализ>>}
\end{center}

\vspace{3cm}

\begin{flushright}
    \large
    \textit{Студент 415 группы}\\
    К.\,И.~Салихова

    \vspace{5mm}

    \textit{Руководитель практикума}\\
    А.\,Ю.~Заночкин
\end{flushright}

\vfill

\begin{center}
    Москва, 2020
\end{center}

\clearpage
\tableofcontents
\clearpage
 \section{Задание №1} \label{task_01}

\begin{enumerate}
        \item Реализовать генератор схемы Бернулли с заданной вероятностью успеха $p$. На основе генератора схемы Бернулли построить датчик для биномиального распределения.
        \item Реализовать генератор геометрического распределения. Проверить для данного распределения свойство отсутствия памяти.
        \item Рассмотреть игру в орлянку --- бесконечную последовательность независимых испытаний с бросанием правильной монеты. Выигрыш $S_n$ определяется как сумма по всем $n$ испытаниям значений $1$ и $-1$ в зависимости от выпавшей стороны. Проиллюстрировать (в виде ломаной) поведение нормированной суммы $Y(i) = \frac{S_i}{\sqrt{n}}$ как функции от номера испытания $i = 1,\,\ldots,\,n$ для одной отдельно взятой траектории. Дать теоретическую оценку для $Y(n)$ при $n\to\infty$.
\end{enumerate}

\subsection{Генератор схемы Бернулли. Датчик биномиального распределения}

\begin{definition}
        \textit{Схемой Бернулли} с заданной вероятностью успеха p называется эксперимент, состоящий из серии испытаний, удовлетворяющих следующим свойствам:
        \begin{enumerate}
            \item Отсутствие взаимного влияния,
            \item Воспроизводимость испытаний (испытания производятся в сходных условиях),
            \item В каждом испытании наблюдается признак, причем вероятность его проявления (успеха) равна $p$.
        \end{enumerate}
\end{definition}
\begin{definition}
        Случайная величина $Y$ , принимающая значение $1$ с вероятностью $p$ и значение $0$ с вероятностью $1-p$, называется \textit{случайной величиной с распределением Бернулли}.
\end{definition}
Будем обозначать такую случайную величину:
$$
    X \sim \mbox{Bern}(p).
$$
\begin{definition}
    Пусть $X_1,\,\ldots\,, X_n$ — набор независимых случайных величин с распределением Бернулли. Тогда, случайная величина:
        $$Y = \sum\limits_{i=1}^n X_i$$
    называется \textit{случайной величиной, имеющей биномиальное распределение с параметрами $n$ и $p$}.
\end{definition}
Будем обозначать такую случайную величину:
$$
    Y \sim \mbox{Bi}(n, p).
$$
Чтобы смоделировать случайную величину $X \sim \mbox{Bern}(p)$, будем генерировать случайную величину $X_0 \sim \mbox{U}[0, 1]$. Тогда если задать $Y$ следующим образом:
\clearpage
$$
Y = 
\begin{cases}
   1, &\text{$X_0 \in [0, p),$}\\
   0, &\text{$X_0 \in [p, 1],$}
 \end{cases}
$$
то $\p(Y = 1) = p, \, \p(Y = 0) = 1 - p.$ То есть $Y$ имеет распределение Бернулли.

Теперь смоделируем случайную величину $Y \sim \mbox{Bi}(n, p)$. Для этого $n$ раз будем генерировать случайную величину $Y_0 \sim \mbox{Bern}(p)$ и просуммируем. Тогда $\p(Y = k) = C_n^k p^k (1-p)^{n-k},$ и $Y$ имеет биномиальное распределение.
\begin{figure}[h]
    \centering
    \includegraphics[width=120mm]{1_1_100000.png}
    \caption{Гистограмма биномиального распределения случайной величины с параметрами $n = 100$, $p = 0.5$ при $100000$ испытаний.}
\end{figure}

\subsection{Датчик геометрического распределения. Его свойство отсутствия памяти}
\begin{definition}
    Случайная величина $Y$, равная количеству неудач до появления первого успеха в схеме Бернулли с параметром $p$, называется \textit{случайной величиной, имеющей геометрическое распределение с параметром $p$}.    
\end{definition}
Будем обозначать такую случайную величину:
$$
    Z \sim \mbox{Geom}(p).
$$
Исходя из определения, чтобы смоделировать случайную величину $Z \sim \mbox{Geom}(p)$, будем несколько раз генерировать случайную величину $Z_0 \sim \mbox{Bern}(p)$, и считать, сколько раз $Z_0$ приняла значение 0 до первого появления значения 1. Тогда $\p(Z=k) = (1-p)^k p$, и $Z$ имеет геометрическое распределение.
\begin{assertion}[Свойство отсутствия памяти]
        Пусть $Z\sim\mbox{Geom}(p)$, тогда для любых $n,\,m\in\N_0$ справедливо:
        $$
                \p(Z > m + n\;|\;Z\geqslant m) = \p(Z> n).
        $$
\end{assertion}
\begin{proof}
        $$
                \p(Z > m+n\;|\;Z\geqslant m)=
                \frac{\p(Z > m + n,\,Z\geqslant m)}{\p(Z\geqslant m)}= \frac{\p(Z > m+ n)}{\p(Z\geqslant m)}=
                \frac{\sum_{i=m+n}^{\infty}(1-p)^ip}{\sum_{i=m}^{\infty}(1-p)^ip}=
        $$
        $$
                = \frac{(1-p)^{m+n}}{(1-p)^m}=
                (1-p)^n.
        $$
        С другой стороны:
        $$
                \p(Z > n) = \sum_{i=n}^{\infty}(1-p)^ip =
                p\frac{(1-p)^n}{p} = (1-p)^n.
        $$
        Таким образом:
        $$
        \p(Z > m + n\;|\;Z\geqslant m) = \p(Z> n).
        $$
\end{proof}
\begin{figure}[h]
        \centering
        \includegraphics[width=120mm]{1_2.png}
        \caption{Гистограмма геометрического распределения случайной величины с параметром $p = 0.3$ при $1000$ испытаний.}
\end{figure}
\begin{figure}[H]
        \noindent
        \centering
        \includegraphics[width=120mm]{отсутствие_памяти.png}
        \caption{Свойство отсутствия памяти геометрического распределения. Здесь задан параметр геометрического распределения $p = 0.1$, а также <<сдвиг>> $m = 10$.}
\end{figure}
\subsection{Игра в орлянку}

Для того, чтобы дать теоретическую оценку для $Y(n)$ при $n \to\infty$, воспользуемся Центральной предельной теоремой.

\begin{theorem}[Центральная предельная теорема]
        Пусть $X_1,\,\ldots,\,X_n,\,\ldots$ есть бесконечная последовательность независимых одинаково распределенных случайных величин, имеющих конечное математическое ожидание $\mu$ и дисперсию $\sigma^2$. Пусть также $S_n = \sum_{i=1}^{n} X_i$. Тогда
        $$
                \frac{S_n - \mu n}{\sigma\sqrt{n}} \longrightarrow \mbox{N}(0,\, 1)
        $$
        по распределению при $n\to\infty$.
\end{theorem}

В нашей задаче $X_i$ - независимые одинаково распределенные дискретные случайные величины, принимающие только два значения: $1$ и $-1$. Поэтому:
$$
        \E\,X_i = 1 \cdot \frac12 - 1 \cdot \frac12 = 0,
$$
$$
        \Var\,X_i = \frac12 \cdot (1 - 0)^2 + \frac12 \cdot (-1 - 0)^2 = 1.
$$

Применяя ЦПТ, получим:
$$
    Y(n) \xrightarrow{dist.} N(0,\,1).
$$
Тогда по <<правилу трех сигм>>:
$$
        -3\leqslant\lim\limits_{n\to\infty}Y(n)\leqslant 3
$$
с вероятностью приблизительно $0,9973$.
\begin{figure}[h]
        \noindent
        \centering \includegraphics[width=120mm]{1_3.png}
        \caption{Поведение суммы $Y(i)$ на отрезке $1 \leqslant i\leqslant 1000$.}
\end{figure}





\section{Задание №2}

\begin{enumerate}
        \item Построить датчик сингулярного распределения, имеющий в качестве функции распределения канторову лестницу. С помощью критерия Колмогорова убедиться в корректности работы датчика.
        \item Для канторовых случайных величин проверить свойство симметричности относительно $\frac12$ ($X$ и $(1 - X)$ распределены одинаково) и самоподобия относительно деления на $3$ (условное распределение $Y$ при условии $Y\in[0,\,\frac13]$ совпадает с распределением $\frac{Y}{3}$) с помощью критерия Смирнова.
        \item Вычислить значение математического ожидания и дисперсии для данного распределения. Сравнить теоретические значения с эмпирическими для разного объема выборок. Проиллюстрировать сходимость.
\end{enumerate}


\subsection{Построение датчика <<канторовой>> случайной величины}

\begin{definition}
        Функция распределения некоторой случайной величины называется \textit{сингулярной}, если она непрерывна и ее множество точек роста имеет нулевую меру Лебега.
\end{definition}
\begin{definition}
        \textit{Канторовым множеством} называется совершенное нигде не плотное множество. 
\end{definition}
Как известно, точка принадлежит канторовому множеству тогда и только тогда, когда в записи этой точки в троичной системе координат нет единиц. Канторова лестница $K(x)$ действует на такие точки по следующему правилу:
$$
K(\{\alpha_1, \ldots, \alpha_n, \ldots\}_3) = \left\{\frac{\alpha_1}{2}, \ldots, \frac{\alpha_n}{2}, \ldots\right\}_2.
$$
Здесь $\{\alpha_1, \ldots, \alpha_n, \ldots\}_3$ - запись аргумента в троичной системе счисления.

Для моделирования воспользуемся методом обращения функции распределения.
\begin{theorem}[Метод обратной функции распределения]
\label{th:inv-method}
        Пусть некоторая функция распределения $F$ имеет обратную $F^{-1}$. Тогда функцией распределения случайной величины
        $$
                \eta = F^{-1}(\xi),
        $$
        где $\xi$~--- равномерно распределенная на отрезке $[0,\,1]$ случайная величина, является $F$.
\end{theorem}
\begin{proof}
        Найдем функцию распределения случайной величины $\eta$:
        $$
                F_\eta(x) =
                \p(\eta \leqslant x) =
                \p(F^{-1}(\xi) \leqslant x) =
                \p(\xi \leqslant F(x)) =
                F(x).
        $$
\end{proof}

Итак, в качестве случайной величины, имеющей равномерное на $[0, 1]$ распределение, возмем следующую случайную величину:
$$
\xi = \sum\limits_{k=1}^{\infty}\frac{\alpha_k}{2^k},
$$
где $\alpha_k \sim \mbox{Bi(1, 0.5)}$. Таким образом мы равновероятно генерируем знаки разложения числа в двоичной системе счисления. Если мы хотим моделировать случайную величину с функцией распределения $K(x)$, то нужно выснить, по какому приниципу действует функция $K^{-1}(\xi)$. Из определение канторовой функции следует, что:
$$
\eta = K^{-1}(\xi) = \{2\alpha_1, \ldots, 2\alpha_n, \ldots\}_3 = \sum\limits_{k=1}^{\infty}\frac{2 \alpha_k}{3^k}.
$$
Вычислим погрешность, которую мы будем совершать, считая вместо бесконечных сумм конечные. Для этого оценим остаток ряда:
$$
        \sum_{k=n}^{\infty} \frac{2\alpha_k}{3^k} \leqslant 2\sum_{k=n}^{\infty}\frac{1}{3^k} = \frac{1}{3^{n-1}} \leqslant \varepsilon.
$$
Чтобы выполнялось последнее неравенство для фиксированного $\varepsilon$ будем выбирать $n$ так, чтобы:
$$
        n \geqslant 1 - \left\lceil\,\log_3 \varepsilon\,\right\rceil \; \forall \varepsilon < 1.
$$
Для того чтобы убедиться в правильности работы датчика, сравним получившуюся функцию распределения канторовой случайной величины с эмперической.
\begin{definition}
        Пусть задана выборка $\xi = (\xi_1,\,\xi_2,\,\ldots,\,\xi_n)$. \textit{Эмперической (выборочной) функцией распределения}, построенной на этой выборке, называется функция $F_n(x)$:
$$
        F_n(x) = \frac{1}{n}
        \sum_{i = 1}^n \mathbb{I}_{(-\infty,\,x)}(\xi_i),
        \quad
        \mbox{где }
        \mathbb{I}_{(-\infty,\,x)}(\xi_i)
        =
        \begin{cases}
                1, & \mbox{при } \xi_i < x, \\
                0, & \mbox{иначе.}
        \end{cases}
$$
Таким образом, $F_n(x)$ равна доле таких значений $\xi_i$, что $\xi_i < x$.
\end{definition}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{2_1.png}
        \caption{Эмпирическая и теоретическая функции распределения канторовой случайной величины $X$ при выборке из $1000$~испытаний.}
\end{figure}


\subsection{Проверка корректности работы датчика}
\begin{definition}
        Пусть в некотором эксперименте доступна наблюдению случайная величина $\xi$, распределение которой $\p$ полностью или частично неизвестно. Тогда любое утверждение относительно $\p$ называется \textit{статистической гипотезой} $H$.
\end{definition}
\begin{theorem}[Критерий согласия Колмогорова]
        Обозначим нулевую гипотезу $H_0$ как гипотезу о том, что выборка подчиняется распределению $F(\xi)$. Введем статистику критерия
$$
        D_n = \sup\limits_{x}|F_n(x) - F(x)|.
$$
        Тогда если гипотеза $H_0$ верна, то $\sqrt{n}D_n$ с ростом $n$ сходится по распределению к случайной величине $K$ с функцией распределения Колмогорова
$$
        F_K(x) = 1 + 2\sum_{i = 1}^{+\infty} (-1)^k e^{-2k^2x^2}.
$$
\end{theorem}

Гипотеза $H_0$ отвергается, если при большом объеме выборки $n$ статистика $\sqrt{n}D_n$ превышает квантиль распределения $K_\alpha =  F_K^{-1}(1 - \alpha)$, заданного \textit{уровня значимости } $\alpha$, и принимается в противном случае.

Но вместо того, чтобы считать значение $K_{\alpha}$, будем получать \textit{p-значение} для нашей статистики: $p_{value} = 1 - F_K(\sqrt{n}D_n)$. Тогда гипотеза принимается при уровне значимости $\alpha$, если $p_{value} > \alpha$.

В следующей таблице показаны результаты проведенных экспериментов. Можно заметить, что гипотеза о равенстве построенной нами функции распределения и эмперической функции распределения отвергается с вероятностью, не превышающей текущий уровень значимости.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Число испытаний &
Размер выборки  &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$1000$
&
$1000$
&
$0,10$
&
$0,9050$
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,9480$
\\
\hline
$1000$
&
$10000$
&
$0,10$
&
$0,9030$
\\
\hline
$1000$
&
$10000$
&
$0,05$
&
$0,9630$
\\
\hline
$10000$
&
$1000$
&
$0,10$
&
$0,9091$
\\
\hline
$10000$
&
$1000$
&
$0,05$
&
$0,9561$
\\
\hline
$10000$
&
$10000$
&
$0,10$
&
$0,9027$
\\
\hline
$10000$
&
$10000$
&
$0,05$
&
$0,9498$
\\
\hline
\end{tabular}
\end{center}
\caption{Частота принятия гипотезы $H_0$ о том, что построенный датчик случайной величины имеет канторову лестницу в качестве функции распределения.}
\end{table}


\subsection{Симметричность <<канторовой>> случайной величины \\ относительно $\frac{1}{2}$}

\begin{assertion}[Свойство симметричности относительно $\frac{1}{2}$]
        Пусть $X$ --- случайная величина, с канторовой лестницей в качестве функции распределения. Тогда верно
$$
        F_X(x) = F_{1-X}(x).
$$
\end{assertion}
\begin{proof}
    Рассмотрим случайную величину $1-X$:
$$
        1 - X
        =
        1 - \sum_{k=1}^\infty\frac{2\alpha_k}{3^k}
        =
        \sum_{k=1}^\infty\frac{2}{3^k} - 2 \sum_{k=1}^\infty\frac{\xi_k}{3^k}
        =
        \sum_{k=1}^\infty\frac{2(1 - \xi_k)}{3^k}
        =
        \sum_{k=1}^\infty\frac{2\eta_k}{3^k}.
$$ 
        Случайные величины $\eta_k = 1 - \xi_k$ имеют распределение Бернулли с параметром 0.5.
        
        Но так как $\alpha_k$ по построению тоже имеют распределение Бернулли с параметром 0.5, можно утверждать, что функции распределения случайных величин $X$ и $1-X$ совпадают.

\end{proof}
Теперь с помощью \textit{критерия Смирнова} проверим, что выборки слуйчаных величин вида $x$ и $1-x$ принадлежат одному распределению. 
\begin{theorem}[Критерий однородности Смирнова]
        Обозначим за нулевую гипотезу $H_0$ гипотезу о том, что две исследуемые выборки объемами $n$ и $m$ с эмперическими функциями распределения $F_n(x)$ и $F_m(x)$ соответственно распределениы по одному закону. Введем статистику критерия
$$
        D_{n,\,m} = \sup\limits_{x}|F_n(x) - F_m(x)|.
$$
Тогда если гипотеза $H_0$ верна, то при увеличении объемов выборок $n$ и $m$ случайная величина $\sqrt{\frac{nm}{n + m}}D_{n,m}$ будет сходиться по распределению к случайной величине $K$ с функцией распределения Колмогорова
$$
       F_K(x) = 1 + 2\sum_{i = 1}^{+\infty} (-1)^k e^{-2k^2x^2}. 
$$
\end{theorem}
Для того, чтобы принять или отклонить гипотезу, также будем вычислять $p-значение$ для нашей статистики: $p_{value} = 1 - F_K\left(\sqrt{\frac{nm}{n+m}}D_{n, m}\right).$
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{2_2_1.png}
        \caption{Свойство симметричности относительно $\frac{1}{2}$ канторовой случайной величины.}
\end{figure}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Размер выборки $x_1$ &
Размер выборки $x_2$ &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$1000$
&
$1000$
&
$0,10$
&
$0,899$
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,949$
\\
\hline
$1000$
&
$10000$
&
$0,10$
&
$0,906$
\\
\hline
$1000$
&
$10000$
&
$0,05$
&
$0,953$
\\
\hline
$10000$
&
$1000$
&
$0,10$
&
$0,900$
\\
\hline
$10000$
&
$1000$
&
$0,05$
&
$0,948$
\\
\hline
$10000$
&
$10000$
&
$0,10$
&
$0,904$
\\
\hline
$10000$
&
$10000$
&
$0,05$
&
$0,947$
\\
\hline
\end{tabular}
\end{center}
\caption{Частота принятия гипотезы $H_0$ о том, что случайные величины $X$ и $(1 - X)$ имеют одинаковое распределение.}
\end{table}


\subsection{Самоподобие <<канторовой случайной величины относительно деления на 3}

\begin{assertion}[Свойство самоподобия относительно деления на 3]
        Пусть $X$ --- случайная величина с канторовой лестницей в качестве функции распределения. Тогда верно
$$
        F_{\frac{X}{3}}(x) = F_{X\,|\,X\in\left[0,\,\frac{1}{3}\right]}(x).
$$
\end{assertion}
\begin{proof}
Легко заметить, что по построению случайная величина $\displaystyle X =  \sum\limits_{k=1}^{\infty}\frac{2\alpha_k}{3^k} \in \left[0, \frac{1}{3}\right]$ тогда и только тогда, когда $\alpha_1 = 0$. Поэтому случайная величина $\displaystyle Y = X|X\in\left[0, \frac{1}{3}\right]$ равна:
$$
\displaystyle
Y = \sum\limits_{k=2}^{\infty} \frac{2\alpha_k}{3^k} = \sum\limits_{k=1}^{\infty} \frac{2\alpha_{k+1}}{3^{k+1}} = \sum\limits_{k=1}^{\infty} \frac{2}{3}\frac{2\alpha_k}{3^k} = \frac{1}{3}X. 
$$
\end{proof}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{2_2_2.png}
        \caption{Свойство самоподобия относительно деления на 3 канторовой случайной величины.}
\end{figure}
Проверим получившийся результат при помощи критерия Смирнова.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Размер выборки $x_1$ &
Размер выборки $x_2$ &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$1000$
&
$1000$
&
$0,10$
&
$0,901$
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,946$
\\
\hline
$1000$
&
$10000$
&
$0,10$
&
$0,906$
\\
\hline
$1000$
&
$10000$
&
$0,05$
&
$0,956$
\\
\hline
$10000$
&
$1000$
&
$0,10$
&
$0,898$
\\
\hline
$10000$
&
$1000$
&
$0,05$
&
$0,955$
\\
\hline
$10000$
&
$10000$
&
$0,10$
&
$0,893$
\\
\hline
$10000$
&
$10000$
&
$0,05$
&
$0,939$
\\
\hline
\end{tabular}
\end{center}
\caption{Частота принятия гипотезы $H_0$ о том, что случайные величины $X\,|\,X\in[0,\,\frac{1}{3}]$ и $\frac{X}{3}$ имеют одинаковое распределение.}
\end{table}

\subsection{Математическое ожидание и дисперсия канторовой случайной величины}

Найдем математическое ожидание и дисперсию случайной величины $X$, имеющей в качестве функции распределения канторову лестницу:
$$
        \E\,X
        =
        \E\sum_{k=1}^{\infty}\frac{2\alpha_k}{3^k}
        =
        \sum_{k=1}^{\infty}\frac{2}{3^k}\E\alpha_k=
        \{\text{так как }\alpha_k \sim Bern(0.5)\} =
        \sum_{k=1}^\infty \frac{2}{3^k}\cdot\frac12
        =
        \frac{\frac13}{1 - \frac13}
        =
        \frac{1}{2}.
$$
$$
        \Var\,X
        =
        \Var\sum_{k=1}^{\infty}\frac{2\xi_k}{3^k}
        = \{\text{так как }\alpha_k \text{ независимы}\} =
        \sum_{k=1}^{\infty}\left(\frac{2}{3^k}\right)^2\Var\,\xi_k
        =
$$
$$
        =\sum_{k=1}^{\infty}\frac{4}{9^k}\cdot\frac12 \cdot\frac{1}{2}
        =
        \frac{\frac19}{1 - \frac19}
        =
        \frac18.
$$

В качестве эмпирических значений построим графики \textit{выборочного среднего} $\overline{X}$ и \textit{несмещенной выборочной дисперсии} $S^2$:
$$
        \overline{X} = \frac{1}{n}\sum_{i=1}^n X_i
        \quad
        \mbox{и}
        \quad
        S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})^2.
$$

\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{2_3_1.png}
        \caption{График выборочного среднего случайной величины~$X$ в зависимости от объема выборки: $400\leqslant n \leqslant 4000$.}
\end{figure}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{2_3_2.png}
        \caption{График несмещенной выборочной дисперсии случайной величины~$X$ в зависимости от объема выборки: $400\leqslant n \leqslant 10000$.}
\end{figure}





\section{Задание №3}

\begin{enumerate}
        \item Построить датчик экспоненциального распределения. 
        Проверить для данного распределения свойство отсутствия памяти. Пусть $X_1,\,X_2,\,\ldots,\,X_n$ независимо экспоненциально распределенные случайные величины с параметрами $\lambda_1,\,\lambda_2,\,\ldots,\,\lambda_n$ соответственно. 
        Найти распределение случайной величины $Y = \min\{\,X_1,\,X_2,\,\ldots,\,X_n\,\}$.
        \item На основе датчика экспоненциального распределения построить датчик пуассоновского распределения.
        \item Построить датчик пуассоновского распределения как предел биномиального распределения.
        С помощью критерия хи-квадрат Пирсона убедиться, что получен датчик распределения Пуассона.
        \item Построить датчик стандартного распределения методом моделирования случайных величин парами с переходом в полярные координаты. Проверить при помощи t-критерия Стьюдента равенство математических ожиданий, а при помощи критерия Фишера --- равенство дисперсий.
\end{enumerate}


\subsection{Построение датчика экспоненциальной случайной величины}

\begin{definition}
        Будем говорить, что случайная величина $X$ \textit{имеет экспоненциальное распределение с параметром $\lambda > 0$}, если ее функция распределения имеет вид
$$
        F_X(x) = 
        \begin{cases}
                1 - e^{-\lambda x},& \mbox{при $x \geqslant 0$,} \\
                0, & \mbox{при $x < 0$.}
        \end{cases}
$$
        Обозначение:
$$
        X \sim \mbox{Exp}(\lambda).
$$
\end{definition}
Для моделирования экспоненциально распределенной случайной величины снова воспользуемся методом обращения функции распределения. Пусть $\xi \sim \mbox{U}[0, 1]$. Тогда по теореме 2.1:
$$
\eta = F_{X}^{-1}(\xi) = -\frac{1}{\lambda}\ln{(1-\xi)} \Rightarrow \eta \sim \mbox{Exp}(\lambda).
$$
\begin{figure}[H]
    \centering
    \includegraphics[width=120mm]{3_1_1.png}
    \caption{Гистограмма экспоненциального распределения с параметром $\lambda = 2$ при $10000$ испытаниях.}
\end{figure}
\subsection{Отсутствие памяти у экспоненциального распределения}

\begin{assertion}[Свойство отсутствия памяти]
        Пусть $X\sim\mbox{Exp\,}(\lambda)$, тогда \forall $t \neq 0$, \forall $s$ справедливо:
$$
        \p(X\geqslant s+t\,|\,X\geqslant t) =
        \p(X \geqslant s).
$$
\end{assertion}

\begin{proof}
        Аналогично тому, как мы поступали в доказательстве свойства отсутствия памяти геометрического распределения:
$$
        \p(X \geqslant s + t\,|\,X\geqslant t) =
        \frac{\p(X \geqslant s + t,\,X\geqslant t)}{\p(X\geqslant t)} =
        \frac{\p(X \geqslant s + t)}{\p(X\geqslant t)}.
$$
Тогда нужно показать, что:
$$
        \p(X\geqslant s+t) = 
        \p(X\geqslant t)\p(X\geqslant s).
$$
Но это равенство равносильно тому, что:
$$
1-F_X(s+t) = (1-F_X(t))(1-F_X(s)) \Rightarrow e^{-\lambda(s+t)} = e^{-\lambda s}e^{-\lambda t}.
$$
Получили тождество, и так как все переходы были равносильными, доказали утверждение.
\end{proof}
\begin{figure}[h]
        \centering
        \includegraphics[width=120mm]{3_1_2.png}
        \caption{Свойство отсутствия памяти у экспоненциального распределения с параметром $\lambda = 1.5$ при $10000$ испытаниях. Сдвиг равен 1.}
\end{figure}

\subsection{Распределение минимума экспоненциальных случайных величин}
Рассмотрим следующую случайную величину:
$$Y = \min\{X_1, X_2, \dots, X_n\}, \text{ где } X_i\sim Exp(\lambda_i) \, \forall i = \overline{1, n}.$$
Вычислим ее функцию распределения:
$$F_Y(x) = \p(Y < x) = \p(\min_{i = \overline{1, n}}{X_i} < x) = $$
$$ = 1 - \p(\min_{i = \overline{1, n}}{X_i} \geqslant x) = 1 - \p(X_1 \geqslant x, \dots, X_n \geqslant x) = 1 - \prod\limits_{i = 1}^{n}\p(X_i \geqslant x) = $$
$$= 1 - \prod\limits_{i = 1}^{n}(1 - F_{X_i}(x)) = 1 - \prod\limits_{i = 1}^{n}e^{\lambda_i x} = 1 - e^{\sum\limits_{i=1}^n \lambda_i x}.$$

Получили, что $Y \sim \mbox{Exp}\left(\sum\limits_{i=1}^n\lambda_i\right).$
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{3_1_3.png}
        \caption{Случайная величина $Y = \min\{X_1, X_2, \dots, X_n\}$ имеет экспоненциальное распределение с параметром $\lambda = \sum\limits_{i=1}^n\lambda_i$. Число испытаний $10000$.}
\end{figure}
\subsection{Построение датчика распределения Пуассона}

\begin{definition}
        Случайная величина $X$ \textit{имеет распределение Пуассона с параметром $\lambda > 0$}, если
$$
        \p(X = k) = \frac{\lambda^k}{k!} e^{-\lambda},
        \quad
        \mbox{где } k = 0, 1, \ldots.
$$
Обозначение:
$$
        X \sim \mbox{Pois}(\lambda).
$$
\end{definition}

\begin{theorem}[О распределении суммы экспоненциальных случайных величин]
    Пусть $\eta_1, \ldots, \eta_n, \ldots$ - независимые случайные величины, имеющие экспоненциальное распределение с параметром $\lambda$, и пусть:
    $$
    \xi = \max{\{n: \eta_1 + \ldots + \eta_n < 1\}}.
    $$
    Тогда случайная величина $\xi$ имеет распределение Пуассона с параметром $\lambda.$ 
\end{theorem}
Доказательство можно найти в [3].

Воспользуемся этим утверждением для моделирования случайной величины с Пуассоновским распределением.
Будем генерировать случайные величины $\eta_k \sim \mbox{Exp}(\lambda)$ до тех пор, пока их сумма не станет больше 1. Тогда, согласно утверждению, случайная величина $k-1$ будет иметь распределение Пуассона с параметром $\lambda$. 

\begin{figure}[h]
        \centering
        \includegraphics[width=120mm]{3_2.png}
        \caption{Распределение Пуассона с параметром $\lambda = 5$ при $10000$ испытаниях.}
\end{figure}

\subsection{Построение датчика распределения Пуассона как предел биномиального распределения}
Для моделирования воспользуемся следующим свойством. При фиксированном $\lambda$, если $n$ - достаточно велико:
$$
\mbox{Bin}\left(n, \frac{\lambda}{n}\right) \approx \mbox{Pois}(\lambda).
$$
Доказательство этого утверждения можно найти в [3].

\subsection{Проверка корректности работы датчика}

С помощью критерия Пирсона проверим, что полученное в предыдущем пункте распределение действительно является распределением Пуассона.

\begin{theorem}[Критерий согласия Пирсона]
        Обозначим нулевую гипотезу $H_0$ как гипотезу о том, что выборка $\xi_1,\,\xi_2\,\ldots\xi_n$ подчиняется закону распределения $\p$. Обозначим за $n_k$ количество элементов в выборке, равных $k$. За $r$ обозначим количество различных элементов выборки. А за $p_k$ --- вероятность выпадения значения в теоретическом распределении $p_k = \p(\xi = k)$. Введем статистику критерия
$$
        X^2_n = n \sum_{k = 1}^r
        \frac{\left(\frac{n_k}{n} - p_k\right)^2}{p_k}.
$$
        Тогда если гипотеза $H_0$ верна, то статистика $X^2_n$ имеет $\chi^2$ распределение c $r-1$ степенью свободы.
\end{theorem}


    Функцию распределения $\chi^2$ c $l$ степеней свободы
$$
    F_{\chi^2_l}(x) = 
    \frac
    {\gamma(\nicefrac{l}{2},\,\nicefrac{x}{2})}
    {\Gamma(\nicefrac{l}{2})}
$$
сложно посчитать аналитически, поэтому воспользуемся встроенными функциями языка Python для вычисления квантилей распределения $\chi^2$. Результаты экспериментов представлены в следующей таблице.
   
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Число испытаний &
Размер выборки  &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$100$
&
$1000$
&
$0,05$
&
$0,940$
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,959$
\\
\hline
\end{tabular}
\end{center}
\caption{Критерий Пирсона. $H_0$ - гипотеза о том, что построенный датчик случайной величины имеет распределение Пуассона.}
\end{table}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{3_3.png}
        \caption{Распределение Пуассона с параметром $\lambda = 5$ построенное как приближение биномиальным распределением с параметрами $p = 0.01, n = 500$ при $10000$ испытаниях.}
\end{figure}

\subsection{Построение датчика нормального распределения методом моделирования парами с переходом в полярные координаты}
\begin{definition}
        Случайная величина $X$ \textit{имеет нормальное распределение с параметрами $\mu$ и $\sigma^2$}, если ее плотность распределения задается формулой
$$
        \rho_X(x)
        =
        \frac{1}{\sqrt{2\pi}\sigma}
        \exp\left\{
                -\frac{(x - \mu)^2}{2\sigma^2}
        \right\}.
$$
        Будем обозначать такие случайные величины
$$
        X \sim \mbox{N}(\mu,\,\sigma^2).
$$
\end{definition}
\begin{definition}
    Случайная величина $X$ \textit{имеет стандартное нормальное распределение}, если $X \sim \mbox{N}(0,\,1)$.
\end{definition}
Пусть $\eta \sim Exp(1), \,\xi \sim U[0, 2\pi],$ тогда случайная величина $Z = \sqrt{2\eta}\sin{\xi}$ имеет стандартное нормальное распределение:
$$
F_Z(x) = \iint\limits_{\{(\eta, \xi)|\sqrt{2\eta}\sin{\xi}<x\}}\frac{e^{-\eta}}{2\pi}d\eta d\xi = \left\{\eta = \frac{r^2}{2}\right\} =\iint\limits_{\{(r, \xi)\,|\,r\sin{\xi}<x\}}\frac{e^{-\frac{r^2}{2}}}{2\pi}r drd\eta = $$
$$= \left\{X = r\cos{\xi}, Y = r\sin{\xi}\right\}
=\iint\limits_{\{(X, Y)|Y<x\}}\frac{e^{-\frac{X^2}{2}}e^{-\frac{Y^2}{2}}}{2\pi}dXdY =
\int\limits_{-\infty}^{+\infty}\frac{e^{-\frac{X^2}{2}}}{\sqrt{2\pi}}dX\int\limits_{-\infty}^x\frac{e^{-\frac{Y^2}{2}}}{\sqrt{2\pi}}dY = $$
$$= \int\limits_{-\infty}^x\frac{e^{-\frac{Y^2}{2}}}{\sqrt{2\pi}}dY
$$
\begin{figure}[h]
        \centering
        \includegraphics[width=120mm]{3_4.png}
        \caption{Стандартное нормальное распределение случайной величины при $10000$ испытаниях.}
\end{figure}
\subsection{Проверка равенства математического ожидания и дисперсии}
\begin{theorem}[Критерий Стьюдента]
        Обозначим нулевую гипотезу $H_0$ как гипотезу о том, что математическое ожидание выборки $X = \{X_1,\,X_2,\,\ldots,\,X_n\}$ равно некоторому известному значению $\mu$. Введем статистику критерия:
$$
        t = \frac{\overline{X} - \mu}{\nicefrac{S_X}{\sqrt{n}}}.
$$
        Тогда если гипотеза $H_0$ верна, то статистика $t$ имеет распределение Стьюдента с $n-1$ степенью свободы.
\end{theorem}

\begin{theorem}[Критерий Фишера]
        Обозначим нулевую гипотезу $H_0$ как гипотезу о том, что дисперсии двух выборок $X = \{X_k\}_{k=1}^n$ и $Y = \{Y_k\}_{k=1}^m$ равны друг другу. Введем статистику критерия
$$
        F = \frac{\hat \sigma_X^2}{\hat\sigma_Y^2},
        \quad
        \mbox{где }
        \hat\sigma^2
        \mbox{--- выборочная дисперсия}.
$$
        Тогда если гипотеза $H_0$ верна, то статистика $F$ имеет распределение Фишера $F(n-1,\,m-1)$.
\end{theorem}
Для того, чтобы удостовериться в корректности построенного датчика:
\begin{enumerate}
    \item С помощью критерия Стьюдента проверим, что математическое ожидание полученной выборки равно нулю.
    \item С помощью критерия Фишера проверим, что дисперсия полученной выборки равна 1. Для этого нужна будет еще одна выборка, у которой дисперсия точно равна 1. Для создания такой выборки будем использовать датчик случайной величины игры в орлянку.  
\end{enumerate}
Для того, чтобы принять или отвергнуть гипотезу будем использовать значения квантилей $K_r$ распределений Стьюдента и Фишера. Посчитаем их с помощью соответствующих встроенных функций языка Python. Тогда гипотеза $H_0$ принимается при уровне значимости $\alpha$, если для значения статистики $S$ выполнено:
$$
K_{\nicefrac{\alpha}{2}} < S < K_{1 - \nicefrac{\alpha}{2}}.
$$
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Число испытаний &
Размер выборки  &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,953$
\\
\hline
$1000$
&
$1000$
&
$0,10$
&
$0,902$
\\
\hline
$1000$
&
$10000$
&
$0,05$
&
$0,956$
\\
\hline
$1000$
&
$10000$
&
$0,10$
&
$0,921$
\\
\hline
$10000$
&
$1000$
&
$0,05$
&
$0,952$
\\
\hline
$10000$
&
$1000$
&
$0,10$
&
$0,8991$
\\
\hline
$10000$
&
$10000$
&
$0,05$
&
$0,950$
\\
\hline
$10000$
&
$10000$
&
$0,10$
&
$0,907$
\\
\hline
\end{tabular}
\end{center}
\caption{Критерий Стьюдента. $H_0$ - гипотеза о том, что построенный датчик случайной величины имеет равное нулю математическое ожидание.}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Число испытаний &
Размер выборки  &
Уровень значимости &
Частота принятия гипотезы
\\
\hline
$1000$
&
$1000$
&
$0,05$
&
$0,991$
\\
\hline
$1000$
&
$1000$
&
$0,10$
&
$0,984$
\\
\hline
$1000$
&
$10000$
&
$0,05$
&
$0,994$
\\
\hline
$1000$
&
$10000$
&
$0,10$
&
$0,9670$
\\
\hline
$10000$
&
$1000$
&
$0,05$
&
$0,996$
\\
\hline
$10000$
&
$1000$
&
$0,10$
&
$0,979$
\\
\hline
$10000$
&
$10000$
&
$0,05$
&
$0,995$
\\
\hline
$10000$
&
$10000$
&
$0,10$
&
$0,980$
\\
\hline
\end{tabular}
\end{center}
\caption{Критерий Фишера. $H_0$ - гипотеза о том, что построенный датчик случайной величины имеет дисперсию, равную единице.}
\end{table}





\section{Задание №4}

\begin{enumerate}
        \item Построить датчик распределения Коши.
        \item На основе датчика распределения Коши с помощью метода фон Неймана построить датчик стандартного нормального распределения. При помощи функции \texttt{normal probability plot} убедиться в корректности работы построенного датчика и обосновать наблюдаемую линейную зависимость.
        \item Сравнить скорость моделирования стандартного нормального распределения в заданиях 3 и 4.
\end{enumerate}


\subsection{Построение датчика распределения Коши}

\begin{definition}
        Случайная величина $X$ \textit{имеет распределение Коши с параметрами $a$ и $b$}, если ее функция распределения имеет вид:
$$
        F_X(x) = \frac{1}{\pi}\arctg\left(\frac{x-a}{b}\right) + \frac{1}{2}.
$$ 
        Обозначение:
$$
        X \sim \mbox{C}(a,\,b).
$$
\end{definition}
Для моделирования распределения Коши снова воспользуемся методом обращения функции распределения. Будем генерировать случайную велиичну $\xi \sim \mbox{U}[0, 1].$ Тогда случайная величина:
$$
\displaystyle
\eta = F_X^{-1}(\xi) = a + b \tg\left(\pi\left(\xi - \frac{1}{2}\right)\right)
$$
будет иметь распределение Коши.
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{4_1.png}
        \caption{Распределение Коши с параметрами $a = 2$, $b = 0.2$ при $1000$ испытаниях.}
\end{figure}

\subsection{Построение датчика стандартного нормального распределения методом фон Неймана}
Плотность стандартного нормального распределения:
$$
\displaystyle
p(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.
$$
Плотность распределения Коши:
$$
\displaystyle
q(x) = \frac{1}{\pi}\frac{b}{(x-a)^2+b^2}.
$$
Будем моделировать стандартное нормальное распределение следующим образом:
\begin{enumerate}
    \item Найдем число $k$ такое, что $\forall x\in\mathbb{R}$ выполнялось: $p(x) \leqslant k q(x).$
    \item Способом, описанным в предыдущем пункте, будем генерировать случайную величину $x \sim  C(a, b)$.
    \item Сгенерируем случайную величину $\displaystyle \eta(x) \sim Bern\left(\frac{p(x)}{kq(x)}\right)$.
    \item Если $\eta = 1$, то $x \sim N(0, 1)$. Если $\eta = 0$, то повторим все пункты алгоритма, начиная со второго.
\end{enumerate}
Легко заметить, что $\eta = 1$ тем чаще, чем ближе значение $\displaystyle \frac{p(x)}{k q(x)}$ к единице. Так как мы хотим увеличить скорость сходимости алгоритма, будем выбирать $k$ так, чтобы:
$$
k = \min_{a, b}\max_{x}\frac{p(x)}{q(x)} = \min_{a, b}\max_{x} \frac{\sqrt{\pi}}{b\sqrt{2}}((x-a)^2 + b^2)e^{-\frac{x^2}{2}}.
$$
Положим $a=0$ и обозначим $g(x) = (x^2 + b^2)e^{-\frac{x^2}{2}}$. Найдем точку максимума этой функции.
$$
g'(x) = -xe^{-\frac{x^2}{2}}(x^2 + b^2) + 2 x e^{-\frac{x^2}{2}} = - e^{-\frac{x^2}{2}} x (x^2 + b^2 - 2).
$$
Поэтому либо $x=0$, либо $x = \pm \sqrt{2-b^2}$:
$$
\displaystyle
k_{a=0} = \min\left\{ \min \left(\frac{b\sqrt{\pi}}{\sqrt{2}}\right), \min \left(\frac{\sqrt{2\pi}}{b}e^\frac{b^2-2}{2}\right)\right\}.
$$
При $a \neq 0$:
$$
        k_{a \neq 0}
        = \min_{a, b}\max_{x}\frac{p(x)}{q(x)} = \min\limits_a \left\{
                \min\limits_{b > \sqrt{2}} \left.
                \frac{p(x)}{q(x)}
                \right|_{x = 0},\,
        \min\limits_{0 < b \leqslant \sqrt{2}} \left.
                \frac{p(x)}{q(x)}
                \right|_{x = \pm \sqrt{2 - b^2}}
        \right\}
        >
        $$
        $$
        >
        \min\limits_a \left\{
                \min\limits_{b > \sqrt{2}}
                \frac{\sqrt{\pi}}{b\sqrt{2}}(a^2 + b^2),\,
        \min\limits_{0 < b \leqslant \sqrt{2}} 
                (\sqrt{2 - b^2} + |a|)
        \right\} = k_{a=0}.
$$
Получили, что $k_{a \neq 0} > k_{a=0}$. Поэтому искомое $k$ достигается при $a=0$.

Обозначим $\displaystyle f(x) = \frac{\sqrt{2\pi}}{b}e^\frac{b^2-2}{2}$. Найдем её минимум по переменной $b$:
$$
\displaystyle
f'(x) = \sqrt{2\pi}e^\frac{b^2-2}{2}\left(1-\frac{1}{b^2}\right) = 0 \Rightarrow b=1.
$$
Итак, оптимальные значения параметров: $\displaystyle a=0, \,b=1, \,k = \sqrt{\frac{2\pi}{e}}$.
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{4_2.png}
        \caption{Стандартное нормальное распределение случайной, полученное методом фон Неймана при $10000$ испытаниях.}
\end{figure}
Для того, чтобы убедиться в корректности построенного нами датчика стандартного нормального распределения, воспользуемся функцией probability plot. Она, получив на вход некоторую выборку, построит график, на оси абсцисс которого будут отложены точки выборки, а на оси ординат - квантили стандартного нормального распределения.

Пусть $X \sim N(\mu, \sigma^2), \, Y \sim N(0, 1)$. Тогда:
$$
F_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\int\limits_{-\infty}^{x}e^{-\frac{(t-\mu)^2}{2\sigma^2}}dt = \left\{\xi = \frac{t-\mu}{\sigma}\right\} = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\frac{x-\mu}{\sigma}}e^{-\frac{\xi^2}{2}}d\xi = F_Y\left(\frac{x-\mu}{\sigma}\right),
$$

Отсюда следует, что любая случайная величина $X \sim \mbox{N}(\mu,\,\sigma^2)$ представима в виде $X = \sigma Y + \mu$, где $Y \sim \mbox{N}(0,\,1)$.

Таким образом, подавая на вход функции probability plot нормально распределенную выборку, мы ожидаем увидеть прямую с угловым коэффициентом $\sigma$ и со сдвигом $\mu$. Графики ниже демонстрируют то, что построенный нами датчик действительно работает правильно. 

\begin{figure}[H]
        \includegraphics[width=0.5\linewidth]{4_2_1.png}
        \includegraphics[width=0.5\linewidth]{4_2_2.png}
        \caption{Результат работы функции probability plot на выборках $X$~(слева) и $0.5 \cdot X + 2$~(справа), где $X$ - выборка, полученная методом фон Неймана.}
\end{figure}
\begin{figure}[H]
        \includegraphics[width=0.5\linewidth]{4_2_3.png}
        \includegraphics[width=0.5\linewidth]{4_2_4.png}
        \caption{Результат работы функции probability plot на выборках $Y \sim \mbox{Geom}(0.4)$ (слева) и $Z \sim \mbox{Exp}(3)$~(справа).}
\end{figure}
\subsection{Сравнение скорости работы метода моделирования парами  и метода фон Неймана}
Сравним, сколько единиц времени потребуется данным двум методам для моделирования выборки размером $N$. Для этого введем обозначения:
\begin{itemize}
    \item $t_f$ - время работы метода фон Неймана,
    \item $t_p$ - время работы метода моделирования парами,
    \item $a$ - время выполнения одной из стандартный арифметических операций,
    \item $b$ - время, необходимое для генерирования одной равномерно, распределенной случайной величины. При этом $b > a$, так как генерирование равномерно распределенной случайной величины включает в себя выполнение нескольких стандартных арифметических операций.
\end{itemize}
Тогда:
$$
t_f = N \sqrt{\frac{2\pi}{e}}(2b + 2a),
$$
$$
t_p = \frac{N}{2}(2b + 6a).
$$
Причем:
$$
  \frac{1}{2}\sqrt{\frac{2\pi}{e}} = \sqrt{\frac{2\pi}{e}} \frac{b}{2b}\leqslant 2\sqrt{\frac{2\pi}{e}}\frac{a+b}{b + 3a} = \frac{t_f}{t_p} = 2\sqrt{\frac{2\pi}{e}}\frac{a+b}{b + 3a} \leqslant \sqrt{\frac{2\pi}{e}} \frac{2b + 2b}{b} = 4 \sqrt{\frac{2\pi}{e}}
$$
Таким образом $\displaystyle \frac{t_f}{t_p} \in [0.76,\, 6.08].$
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{4_3.png}
        \caption{Сравнение времени работы методов моделирования парами и фон Неймана в зависимости от объема выборки.}
\end{figure}




\section{Задание №5}

\begin{enumerate}
        \item Пусть $X \sim \mbox{N}(\mu,\,\sigma^2)$. Убедиться в справедливости ЗБЧ и ЦПТ, то есть исследовать поведение суммы $S_n$ и эмперического распределения случайной величины
$$
        \sqrt{n}
        \left(
                \frac{S_n}{n} - \mu
        \right).
$$
        \item Считая $\mu$ и $\sigma^2$ неизвестными, для пункта 1 построить доверительные интервалы для среднего и дисперсии.
        \item Пусть $X \sim \mbox{C}(a,\,b)$ имеет распределение Коши со сдвигом $a$ и масштабом $b$. Проверить эмперически, как ведут себя суммы $\displaystyle \frac{S_n}{n}$. Результат объяснить, а также найти закон распределения данных сумм.
\end{enumerate}


\subsection{Закон больших чисел. Центральная предельная теорема.}

\begin{theorem}[Закон больших чисел]
        Пусть $X_1,\,X_2,\,\ldots,\,X_n,\,\ldots$ ---  последовательность независимых одинаково распределенных случайных величин, определенных на одном вероятностном пространстве $(\Omega,\, \mathcal{F},\, \p)$, с конечным первым моментом, равным $\E\,X_i = \mu$. Обозначим $S_n = \sum_{i = 1}^{n} X_i$. Тогда
$$
        \frac{S_n}{n} \xrightarrow{\p} \mu,
$$
то есть
$$
        \forall \varepsilon > 0
        \quad
        \lim\limits_{n\to\infty}
        \p\left(\,
        \left|
                \frac{S_n}{n} - \mu
        \right|
        < \varepsilon
        \right)
        = 1.
$$
\end{theorem}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_1_1.png}
        }
        \caption{Выполнение ЗБЧ для нормально распределенной случайной величины с параметрами $\mu = 2$, $\sigma = 3$. Выборка объема $10^5$.}
\end{figure}

\begin{theorem} [Центральная предельная теорема]
        Пусть $X_1,\,X_2,\,\ldots,\,X_n,\,\ldots$ ---  последовательность независимых одинаково распределенных случайных величин, определенных на одном вероятностном пространстве $(\Omega,\, \mathcal{F},\, \p)$, с конечным первым моментом, равным $\E\,X_i = \mu$ и конечной дисперсией $\Var\,X_i = \sigma^2 \neq 0$. Обозначим $\displaystyle Y_n = \frac{S_n - \mu n}{\sigma \sqrt{n}}$, тогда
$$
        Y_n \xrightarrow{dist.} \mbox{N}(0,\,1),
$$
то есть
$$
        \lim\limits_{n\to\infty}\p(Y_n < x) =
        \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{x} e^{-\frac{t^2}{2}}\,dt
        = F_N(x).
$$
\end{theorem}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_1_2.png}
        }
        \caption{Выполнение ЦПТ для нормально распределенной случайной величины с параметрами $\mu = 2$, $\sigma = 3$. Выборка объема $10^5$.}
\end{figure}

\subsection{Построение доверительных интервалов для среднего и дисперсии нормальной случайной величины.}

Пусть теперь $X_1,\,X_2,\,\ldots,\,X_n$ независимые одинаково распределенные случайные величины с некоторым распределением $\mbox{N}(\mu, \sigma^2)$. Будем считать параметры $\mu,\, \sigma^2$ неизвестными.

\begin{theorem}
        Случайная величина
$$
        T = \sqrt{n} \cdot \frac{\overline X - \mu}{s},
$$
        где $\overline X$ --- выборочное среднее, а $s$ --- несмещенное выборочное стандартное отклонение, имеет распределение Стьюдента с $(n-1)$ степенью свободы.
\end{theorem}

Воспользуемся этим утверждением для построения доверительного интервала для неизвестного математического ожидания $\mu$. Пусть $K_r$ - квантиль порядка $r$ распределения Стьюдента с $n-1$ степенью свободы. Тогда по утверждению теоремы и, исходя из симметричности распределения Стьюдента:
$$
\p(-K_{1-\frac{r}{2}} \leqslant T \leqslant K_{1-\frac{r}{2}}) = 1-r \Rightarrow 
$$
$$
\p(\overline X - \frac{s}{\sqrt{n}} K_{1 - \frac{r}{2}} \leqslant \mu \leqslant \overline X + \frac{s}{\sqrt{n}} K_{1 - \frac{r}{2}})
        =
        1 - r.
$$
При малых $r$ получим искомый доверительный интервал:
$$
\mu \in \left[\overline X - \frac{s}{\sqrt{n}} K_{1 - \frac{r}{2}}, \, \overline X + \frac{s}{\sqrt{n}} K_{1 - \frac{r}{2}}\right].
$$
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_2_1.png}
        }
        \caption{Доверительный интервал для математического ожидания нормально распределенной случайной величины: $\mu = 2$, $\sigma^2 = 9$.}
\end{figure}

\begin{theorem}[Фишер]
        Случайная величина
$$
        H = \frac{s^2}{\sigma^2}(n - 1)
$$
        имеет распределение хи-квадрат с $(n-1)$ степенью свободы.
\end{theorem}
Аналогичным образом, используя данное утверждение, найдем доверительный интервал для дисперсии нормального распределения. Пусть $K_r$ - квантиль порядка $r$ распределения $\chi^2$ с $n-1$ степенью свободы. Тогда по утверждению теоремы:
$$
\p(K_{1+\frac{r}{2}} \leqslant H \leqslant K_{1-\frac{r}{2}}) = r \Rightarrow 
$$
$$
\p\left(\frac{s^2}{\chi^2_{\frac{1 + \alpha}{2}}}
                (n - 1) \leqslant \sigma^2 \leqslant \frac{s^2}{\chi^2_{\frac{1 - \alpha}{2}}}
                (n - 1)\right)
        =
        r.
$$
При $r$, близких к единице, получим искомый доверительный интервал:
$$
\sigma^2 \in \left[\frac{s^2}{\chi^2_{\frac{1 + \alpha}{2}}}
                (n - 1), \, \frac{s^2}{\chi^2_{\frac{1 - \alpha}{2}}}
                (n - 1)\right].
$$
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_2_2.png}
        }
        \caption{Доверительный интервал для дисперсии нормально распределенной случайной величины: $\mu = 2$, $\sigma^2 = 9$.}
\end{figure}

\subsection{Поведение частичных сумм распределения Коши}
Покажем, что суммы $\displaystyle\frac{S_n}{n}$ имеют распределение Коши.
Пусть $X_k \sim \mbox{C}(a, b)$. Тогда ее характеристическая функция:
$$
\varphi_X_k(x) = e^{aix - b|x|}.
$$
Теперь найдем характеристическую функцию случайной величины $\displaystyle\frac{S_n}{n}$:
$$
\displaystyle
\varphi_\frac{S_n}{n}(x) = \varphi_\frac{\sum\limits_{k=1}^n X_k}{n}(x) = \left\{\varphi_{aX}(x) = \varphi_X(ax)\right\} = \varphi_{\sum\limits_{k=1}^n X_k}\left(\frac{x}{n}\right) = 
$$
$$
\displaystyle
= \prod\limits_{k=1}^n\varphi_{X_k}\left(\frac{x}{n}\right) = \left(\varphi_{X_1}\left(\frac{x}{n}\right)\right)^n = \left(e^{\frac{aix}{n}-|\frac{x}{n}|b}\right)^n = e^{aix - b|x|} = \varphi_{X_1}(x).
$$
Так как характеристическая функция случайной величины однозначно характеризует ее распределение, получим, что случайная величина $\displaystyle\frac{S_n}{n}$ имеет распределение Коши.

Известно, что математическое ожидание случайной величины, имеющей рапределение Коши, не существует, так как значение:

$$
        \E\,X =
        \frac{b}{\pi}
        \int\limits_{-\infty}^{+\infty} \frac{1}{(x - a)^2 + b^2}\,dx
$$
не определено. Таким образом, предположения закона больших чисел не выполнены.
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_3_1.png}
        }
        \caption{Распределения частичных сумм $S_{100}$ случайной величины с распределением Коши с параметрами $a = 0$, $b = 1.5$. Выборка объемом $1000$.}
\end{figure}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{5_3_2.png}
        }
        \caption{Невыполнение закона больших чисел для распределения Коши с параметрами $a = 5$, $b = 1$.}
\end{figure}



\section{Задание №6}
\begin{enumerate}
        \item Посчитать интеграл
$$
        \int\limits_{-\infty}^{+\infty}
        \int\limits_{-\infty}^{+\infty}
        \ldots
        \int\limits_{-\infty}^{+\infty}
        \frac
        {
                e^{-\left(x_1^2 + \ldots +x_{10}^2 + \frac{1}{2^7 \cdot x_1^2 \cdot \ldots \cdot x_{10}^2}\right)}
        }
        {
                x_1^2 \cdot \ldots \cdot x_{10}^2
        }
        \, dx_1 dx_2 \ldots dx_{10}
$$
        \begin{itemize}
                \item[-] методом Монте--Карло;
                \item[-] методом квадратур, сводя задачу к вычислению собственного интеграла Римана.
        \end{itemize}
        \item Для каждого случая оценить точность вычислений.
\end{enumerate}


\subsection{Метод Монте--Карло}
Обозначим:
$$
f(x) = \frac
                {
                        e^{-\left(x_1^2 + \ldots +x_{10}^2 + \frac{1}{2^7 \cdot x_1^2 \cdot \ldots \cdot x_{10}^2}\right)}
                }
                {
                        x_1^2 \cdot \ldots \cdot x_{10}^2
                }
$$
Пусть с.в. $X$ имеет плотность распределения $\rho(x)$. Тогда:
$$
\int\limits_{\mathbb{R}^n}f(x) dx = \int\limits_{\mathbb{R}^n}\dfrac{f(x)}{\rho(x)}\rho(x) dx = \int\limits_{\mathbb{R}^n} \tilde f(x) \rho(x) dx = \mathbb{E}\tilde f(X) \approx \dfrac{\sum\limits_{i=1}^{n}\tilde f(x^i)}{n}.
$$

Пусть $X \sim N(O, I), O, I \in \mathbb{R}^{10 \times 10}$, тогда плотность распределения этой случайной величины:
$$
\rho(x) = \frac{1}{(2\pi)^5}e^{-\frac{x_1^2 + \dots + x_{10}^2}{2}}.
$$
Кроме того: 
$$
\tilde f(x) = \frac{f(x)}{\rho(x)} = (2\pi)^5\dfrac{e^{-\left(\frac{1}{2^7 \cdot x_1^2 \cdot \dots \cdot x_{10}^2} + \frac{x_1^2 + \dots + x_{10}^2}{2}\right)}}{x_1^2\cdot \dots \cdot x_{10}^2}
$$

Таким образом, мы можем посчитать значение данного интеграла $I$ следующим образом:
$$
I = \mathbb{E}\tilde f(X) \approx \dfrac{\sum\limits_{i=1}^{n}\tilde f(x^i)}{n}, \quad
        \mbox{где $x^i = (x_1^i,\,\ldots,\,x_{10}^i)$, $x_k^i\sim\mbox{N}(0,\,1)$, $k = \overline{0,\,10}$.}
$$

Чтобы получить точность метода Монте--Карло, воспользуемся ЦПТ. В обозначениях Теоремы 5.2:
$$
        \p(|Y_n - I| < \varepsilon)
        =
        \p\left( \left| \frac{S_n - nI}{n} \right| < \varepsilon \right)
        =
        \p\left( \left| \frac{S_n - nI}{\sqrt{n}\sigma_n} \right| < \frac{\varepsilon\sqrt{n}}{\sigma_n} \right)
        =
$$
$$
        =
        \p\left( \frac{S_n - nI}{\sqrt{n}\sigma_n}  < \frac{\varepsilon\sqrt{n}}{\sigma_n} \right)
        -
        \p\left( \frac{S_n - nI}{\sqrt{n}\sigma_n}  < -\frac{\varepsilon\sqrt{n}}{\sigma_n} \right)
        \approx
$$
$$
        \approx F\left(\frac{\varepsilon\sqrt{n}}{\sigma_n}\right) - F\left(-\frac{\varepsilon\sqrt{n}}{\sigma_n}\right)
        =
        2 F\left(\frac{\varepsilon\sqrt{n}}{\sigma_n}\right) - 1.
$$
Пусть $K_r$ - квантиль порядка $r$ стандартного нормального распределения. Тогда:
$$
\varepsilon = K_{1 - \nicefrac{r}{2}}\cdot \frac{\sigma_n}{\sqrt{n}}.
$$
Результаты экспериментов представлены в следующей таблице.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Число испытаний &
Время работы(с)  &
Погрешность &
Результат
\\
\hline
$10^4$
&
$0.109375$
&
$11.412638$
&
$128.196599$
\\
\hline
$10^5$
&
$0.046875$
&
$3.590046$
&
$122.308228$
\\
\hline
$10^6$
&
$0.437500$
&
$1.143693$
&
$124.793092$
\\
\hline
$10^7$
&
$3.984375$
&
$0.360412$
&
$124.483013$
\\
\hline
$10^8$
&
$83.687500$
&
$0.113965$
&
$124.657149$
\\
\hline
\end{tabular}
\end{center}
\caption{Результаты подсчета данного интеграла методом Монте--Карло.}
\end{table}


\subsection{Метод квадратур}
После замены переменных: $x_i = tg\left(\dfrac{\pi}{2}t_i\right), \,t_i\in[0, 1]$ получим:
$$
I = \left(\frac{\pi}{2}\right)^{10}\int\limits_0^1\dots\int\limits_0^1\frac{exp\left\{-\left[\sum\limits_{i=1}^{10}tg\left(\dfrac{\pi}{2}t_i\right)^2 + \frac{1}{2^7 \prod\limits_{i=1}^{10}tg\left(\dfrac{\pi}{2}t_i\right)^2}\right]\right\}}{\prod\limits_{i=1}^{10}tg\left(\dfrac{\pi}{2}t_i\right)^2 \cdot \prod\limits_{i=1}^{10}cos\left(\dfrac{\pi}{2}t_i\right)^2}dt_1\,dt_2\,\dots\,dt_{10}.
$$
Воспользуемся методом прямоугольника. Равномерно разобьем отрезок $[0, 1]$ на $N$ частей и посчитаем величину:
$$
I_n = \frac{1}{N^{10}}\sum\limits_{i_1=1}^{N}\dots\sum\limits_{i_{10}=1}^N f\left(\frac{i_1}{N},\dots,\frac{i_{10}}{N}\right)
$$
Известно, что погрешность метода прямоугольников на равномерной сетке отрезка $[0, 1]$ равна:
$$
\varepsilon = \frac{h^2}{24}\sum\limits_{i, j = 1}^{10}\max|f''_{x_i, x_j}|, \, h = \frac{1}{N}.
$$
Результаты экспериментов представлены в следующей таблице.
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Число разбиений &
Время работы(с)  &
Результат
\\
\hline
$3$
&
$2.062600$
&
$0.053938$
\\
\hline
$4$
&
$32.015625$
&
$0.086482$
\\
\hline
$5$
&
$342.468750$
&
$0.130859$
\\
\hline
$6$
&
$2176.734375$
&
$0.125344$
\\
\hline
\end{tabular}
\end{center}
\caption{Результаты подсчета интеграла методом квадратур.}
\end{table}

\section{Задание №7}

\begin{enumerate}
        \item Методом случайного поиска найти минимальное значение функции
$
        f
$ 
        на множестве 
$
        A = \{x_1, x_2 : x_1^2 + x_2^2 \leqslant 1\}
$
        , т.е.
$
        y = \min\limits_{x \in A} f(x)
$
        , где 
$$
        f(x) = x_1^3\sin\left(\frac{1}{x_1}\right) +10x_1 x_2^4\cos\left(\frac{1}{x_2}\right)
$$
        при $x_1 \neq 0$ и $x_2 \neq 0$, функция доопределяется по непрерывности при $x_1 = 0$ или $x_2 = 0$.
        
        \item Методом имитации отжига найти минимальное значение функции Розенброка $g$ в пространстве $\R^2$, где 
$$
        g(x) = (x_1-1)^2+100(x_2-x_1^2)^2.
$$

        \item Оценить точность. Сравнить результаты со стандартными методми оптимизации.
\end{enumerate}


\subsection{Метод случайного поиска}
Для поиска минимального значения функции $f(x)$ будем $n$ раз разыгрывать случайные величины $x_1$ и $x_2$, считать для каждой пары значение функции $f(x_1, x_2)$ и затем из полученных значений выберем наименьшее.

Пусть $(x_1, x_2)$ равномерно распределены на единичном круге, тогда:
$$
\mathbb{P}((x_1, x_2) \in A) = \frac{1}{\pi}\iint\limits_{x_1^2 + x_2^2 \leqslant 1} dx_1\,dx_2 = \left\{x_1 = r\cos \varphi, \,x_2 = r\sin\varphi \right\} = \frac{1}{\pi}\int\limits_0^1 r\,dr \int\limits_0^{2\pi}d\varphi = \int\limits_0^{2\pi}\frac{1}{2\pi}d\varphi\int\limits_0^1dr^2.
$$
Сделаем замену:
$$
u = r^2 > 0 \Rightarrow r = \sqrt{u}, \, u \in [0, 1].
$$
Тогда:
$$
\mathbb{P}((x_1, x_2) \in A) = \int\limits_0^{2\pi}\frac{1}{2\pi}d\varphi\int\limits_0^1du.
$$
Значит, совместное распределение $x_1$ и $x_2$ совпадает с совместным распределением следующих случайных величин: $u \sim \mbox{U}[0, 1], \, \varphi \sim \mbox{U}[0, 2\pi].$

С учетом производимых замен переменных получим:
\begin{equation*}
 \begin{cases}
   x_1 = \sqrt{u} \,cos \varphi, 
   \\
   x_2 = \sqrt{u} \,sin\varphi,
 \end{cases}
\end{equation*}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{7_1.png}
        }
        \caption{$f(x) = x_1^3\sin\left(\frac{1}{x_1}\right) +10x_1 x_2^4\cos\left(\frac{1}{x_2}\right).$ }
\end{figure}
Пусть $(x_1^*, x_2^*)$ - теоретическая точка минимума, $(x, y)$ - точка минимума, полученная методом случайного поиска. Так как $f$ непрерывна, она липшицева. Поэтому для $f$ справедлива оценка ([9]):
$$
|f(x_1^*, x_2^*) - f(x, y)| \leqslant \max_{(x_1, x_2) \in A}|\nabla f|\,|(x_1^*, x_2^*) - (x_1, x_2)|.
$$
Для данной функции $f$:
$$
\left|\frac{\partial f}{\partial x_1}\right| = \left|3x_1^2\sin{\frac{1}{x_1}} - \frac{1}{x_1^2} x_1^3 \cos{\frac{1}{x_1}} + 10 x_2^4 \cos{\frac{1}{x_2}}\right| \leqslant |3x_1^2 -  x_1 + 10 x_2^4| =
$$
$$
= |3x_1^2 - x_1 + 10(1-x_1^2)^2| = |3x_1^2 - x_1 + 10(1 - 2x_1^2 + x_1^4)| =$$
$$= |10x_1^4 - 17x_1^2 - x_1 + 10| \leqslant 11
$$
$$
\left|\frac{\partial f}{\partial x_2}\right| = \left|40 x_1 x_2^3 \cos{\frac{1}{x_2}} + 10 \frac{1}{x_2^2} x_1 x_2^4 \sin{\frac{1}{x_2}} \right| \leqslant |40 x_1 x_2^3 + 10 x_1 x_2^2| =
$$
$$
= \left|40 x_1(1-x_1^2)^\frac{3}{2} + 10x_1(1 - x_1^2)\right| \leqslant 17
$$
$$
|\nabla f| = \sqrt{\left(\frac{\partial f}{\partial x_1}\right)^2 + \left(\frac{\partial f}{\partial x_2}\right)^2} \leqslant \sqrt{11^2 + 17^2} \leqslant 21.
$$

Теперь оценим $|(x^*, y^*) - (x, y)|$. Вероятность того, что точка $(x, y)$ попадет в $\varepsilon$-окрестность теоретической точки минимума $(x^*, y^*)$:
$$
\p((x, y) \in B_\varepsilon(x^*, y^*)) = \frac{\pi \varepsilon^2}{\pi} = \varepsilon^2.
$$
Так как функция $f$ четна по $x_2$, точек минимума будет 2: $(x^*, y^*), \, (x^*, -y^*)$. Тогда, если $(x^*, y^*)$ принадлежит границе множества А (то есть у каждой точки минимума множеству принадлежит только половина окрестности), то искомая вероятность равна $\varepsilon^2$. Если же $(x^*, y^*)$ лежит внутри множества А, то искомая вероятность равна $2\varepsilon^2$.

Взяв самую маленькую вероятность из этих двух, получим, что вероятность того, что хотя бы одна из $n$ точек $(x, y)$ попадет в окрестность точки $(x^*, y^*)$ равна $p = n\varepsilon^2.$ Таким образом:
$$|(x^*, y^*) - (x, y)| \leqslant \varepsilon = \sqrt{\frac{p}{n}}$$
Итогова оценка погрешности алгоритма случайного поиска:
$$
f(x^*, y^*) - f(x, y) \leqslant 21\sqrt{\frac{p}{n}}.
$$


\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Число генераций &
Результат  &
Погрешность
\\
\hline
$100$
&
$-1.288473$
&
$2.089474$
\\
\hline
$1000$
&
$-1.288486$
&
$0.660750$
\\
\hline
$10000$
&
$-1.288489$
&
$0.208947$

\\
\hline
\end{tabular}
\end{center}
\caption{Результат работы метода случайного поиска, $p = 0.99$.}
\end{table}

\subsection{Метод имитации отжига}

Пусть ${t_i}$ - некоторая убывающая последовательность, сходящаяся к нулю. Метод имитации отжига работает следующим образом:
\begin{enumerate}
    \item Новый сосед $x^*$ генерируется как нормально распределенная случайная величина со средним $x_i$ и дисперсией $\sigma^2 t_i$
    \item Если $\Delta g = g(x_{i+1}) - g(x_i) < 0$, то повторяем алгоритм, начиная с пункта 1 с новой точкой $x_{i+1} = x^*$.
    \item Если $\Delta g > 0$, то продолжаем алгоритм с новой точкой $x_{i+1} = x^*$ с вероятностью $p(x^*,\,x^{i}) =\exp\left(-\frac{g(x^*) - g(x^i)}{t_i}\right)$.
\end{enumerate}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{7_3.png}
        }
        \caption{Погрешность метода случайного поиска. $p = 0.99.$}
\end{figure}
\begin{figure}[H]
       \noindent
        \centering
        {
                \includegraphics[width=120mm]{7_2.png}
        }
        \caption{$g(x_1,\,x_2) = (x_1-1)^2+100(x_2-x_1^2)^2$.}
\end{figure}
Результаты экспериментов продемонстрированы в следующей таблице:

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Число генераций &
Время работы(с) &
Результат 
\\
\hline
$100$
&
$0.140625$
&
$0.022022$
\\
\hline
$1000$
&
$1.203125$
&
$0.004591$
\\
\hline
$10000$
&
$13.484375$
&
$0.0315778$
\\
\hline
\end{tabular}
\end{center}
\caption{Результат работы метода имитации отжига. $t_{i+1} = 0.99 t_i, \,\sigma = 1$.}
\end{table}



\section{Задание №8}

Применить метод Монте--Карло к решению первой краевой задачи для двумерного уравнения Лапласа в единичном круге:
$$
        \left\{
\begin{array}{lcr}
        \Delta u=0, \, (x,y)\in D,
\\
        u|_{\delta D}=f(x,y),
\\
        u\in C^2(D), \, f \in C(\delta D),
\\
        D = \{\, x,y \,:\, x^2+y^2 \leqslant 1 \,\}.
\end{array}
        \right.
$$

Для функции $f(x,y)=x^2-y^2$ найти аналитическое решение и сравнить с полученным по методу Монте--Карло.

\subsection{Алгоритм решения задачи}
Пусть $u(x, y)$ - искомое решение уравнения Лапласа.
\begin{itemize}
    \item На равномерной сетке множества $D$ выделим внутренние и граничные точки. Будем называть узел сетки~$(i, j)$ \textit{внутренним}, если он и все четыре соседних с ним узла: $(i-1, j), \ (i + 1, j), \ (i, j - 1), \ (i, j + 1)$ принадлежат области~$D + \delta D$; в противном случае узел $(i, j)$, принадлежащий $D + \delta D$, будем называть \textit{граничным}.
    \item В граничных точках положим: $u(x, y) = f(x, y)$.
    \item Во внутренних точках с вероятностью $\frac{1}{4}$ будем переходить в одну из соседних точек до тех пор, пока не окажемся в одной из граничных точек. Для каждой внутренней точки $(x, y)$ повторим этот алгоритм $n$ раз, и положим в данной внутренней точке: $u(x, y) = \frac{1}{n}\sum\limits_{i=1}^n f(x_i, y_i)$, где $(x_i, y_i)$ - посещенные нами граничные точки.
\end{itemize}

Убедимся в корректности алгоритма, сравнив полученные результаты с аналитическим решением для функции $f(x, y) = x^2 - y^2$.

Будем искать аналитическое решение в виде $u(x, y) = Ax^2 + By^2 + C.$ Тогда:
$$
u_{xx} + u_{yy} = 2A + 2B = 0 \Rightarrow A + B = 0\\
A(1 - y^2) + B y^2 + C = 1 - 2y^2 \Rightarrow A + C = 1,\, A-B = 2
$$
То есть: $A = 1, B = -1, C = 0.$ Аналитическое решение: $u(x, y) = x^2 - y^2.$

\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{8_1.png}
        }
        \caption{Аналитическое решение задачи $u(x, y) = x^2 - y^2.$}
\end{figure}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{8_2.png}
        }
        \caption{Решение задачи методом Монте--Карло. Сетка с шагом $0.02$.}
\end{figure}


\clearpage
\section{Задание №9}

Рассмотреть два вида процессов:
\begin{itemize}
        \item Винеровский процесс $W(t)$, $t \in [0,\,1]$, $W(0) = 0$.
        \item Процесс Орнштейна--Уленбека $X(t)$, $t \in [0,\,1]$, $X(0) = X_0$, то есть стационарный гауссовский процесс. Начальные значения $X_0$ генерируются случайным образом так, чтобы получееный процесс был стационарным.
\end{itemize}
Для данных гауссовских процессов:
\begin{enumerate}
        \item Найти ковариационную функцию и переходные вероятности;
        \item Моделировать независимые траектории процесса с данными переходными вероятностями методом добавления разбиения отрезка;
        \item Построить график траектории, не соединяя их ломанной, с целью получения визуально непрерывной линии.
\end{enumerate}


\subsection{Винеровский процесс}

\begin{definition}
        Рассмотрим вероятностное пространство $(\Omega,\,\mathcal{F},\,\p)$. Тогда назовём \textit{случайным процесом} параметризированное семейство $\{\,W_t\,\}_{t \in T}$ случайных величин 
$$
        W_t:\Omega\to\R,\;t\in T,\;T \subset [0,\,+\infty).
$$
\end{definition}

\begin{definition}
        Будем называть случайный процесс $\{\,W_t\,\}_{t\in T}$ \textit{гауссовским}, если для любых $t_0,\,t_1,\,\ldots,\,t_n\in T$ соответствующий случайный вектор $w = (W_{t_1},\,W_{t_2},\,\ldots,\,W_{t_n})$ имеет многомерное нормальное распределение, то есть имеет плотность
$$
        \rho(W_{t_1},\,\ldots,\,W_{t_n})
=
        \frac{1}{(2\pi)^{\nicefrac{n}{2}}|R|^{\nicefrac{1}{2}}}
\cdot
        \exp\left\{
-\frac{1}{2} \cdot
\langle\, R^{-1}(w - m),\, w - m\,\rangle
        \right\},
$$
        где $m = (m_1,\,m_2,\,\ldots,\,m_n)^\T$~--- вектор средних, а $R \in \R^{n \times n}$~--- ковариационная матрица: $R = \|\mbox{cov}(t_i,\,t_j)\|_{i,\,j}$, $R = R^\T > 0$.
\end{definition}

\begin{definition}
        Определим \textit{винеровский процесс} как гауссовский процесс на отрезке $[0,\,1]$ с нулевым средним и ковариационной функцией $\mbox{cov}(W(t_i),\,W(t_j)) = \min\{t_i,\,t_j\}$.
\end{definition}

Вычислим переходные вероятности для винеровского процесса.

Пусть $t = t_1 + \alpha(t_2 - t_1), \, \alpha \in (0, 1)$, тогда $t \in [t_1, t_2].$

Пусть $\tilde{x} = [x_1, x_2]^T, \, \hat{x} = [x_1, x, x_2]^T$. Тогда из определения гауссовского процесса:
$$
\rho_{W(t_1), W(t_2)}(\tilde{x}) = \frac{1}{2\pi\sqrt{|R_2|}} e^{-\frac{1}{2} \tilde{x}^T R_2^{-1} \tilde{x}},
$$
$$
\rho_{W(t_1), W(t), W(t_2)}(\hat{x}) = \frac{1}{(2\pi)^{\frac{3}{2}}\sqrt{|R_3|}} e^{-\frac{1}{2} \hat{x}^T R_3^{-1} \hat{x}},
$$
где $R_2 = \begin{bmatrix} t_1& t_1\\ t_1& t_2 \end{bmatrix}, \, R_3 = \begin{bmatrix} t_1& t_1& t_1\\ t_1& t& t\\ t_1& t& t_2 \end{bmatrix}$ -  ковариационные матрицы.

Тогда $|R_2| = t_1(t_2 - t_1), \, |R_3| = t_1(t - t_1)(t_2 - t)$. 

Обратные матрицы:
$$
R_2^{-1} = \begin{bmatrix} \frac{t_2}{t_1(t_2 - t_1)}& -\frac{1}{t_2 - t_1}\\ -\frac{1}{t_2 - t_1}& \frac{1}{t_2 - t_1} \end{bmatrix}, \, R_3^{-1} = \begin{bmatrix} \frac{t}{t_1(t-t_1)}& -\frac{1}{t-t_1}& 0\\ -\frac{1}{t-t_1}& \frac{t_2-t_1}{(t_2 - t)(t-t_1)}& -\frac{1}{t_2-t}\\0& -\frac{1}{t_2-t}& \frac{1}{t_2-t} \end{bmatrix}.
$$

Тогда:
$$
\rho_{W(t)}(x | W(t_1)=x_1, \,W(t_2) = x_2) = \frac{1}{\sqrt{2\pi(1-\alpha)(t_2 - t_1)}}\exp\left\{-\frac{(x-(1-\alpha)x_1 + \alpha x_2)^2}{2\alpha(1-\alpha)(t_2 - t_1)}\right\}.
$$
Таким образом:
$$
        W(t)
\sim
        \mathrm{N}
        (
          (1 - \alpha)
          x_1
          +
          \alpha
          x_2
        ,\,
          \alpha
          (1 - \alpha)
          (t_2 - t_1)
        ).
$$
Для моделирования винеровского процесса зададим начальную инициализацию: $W(t_0=0) = 0, \, W(t_1=1) \sim \mbox{N}(0, 1).$ Далее будем рекурсивно вызывать функцию, которая разбивает отрезок $[t_0, t_1]$  в отношении $\alpha=0.2$ и в новой точке $t$ будем вычислять значение $W(t)$ как случайную величину с распределением $\mathrm{N}
        (
          (1 - \alpha)
          x_1
          +
          \alpha
          x_2
        ,\,
          \alpha
          (1 - \alpha)
          (t_2 - t_1)
        ).$
        
Остановим алгоритм по достижении заданной точности $t_{k+1} - t_k < \varepsilon$.

\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{9_1.png}
        }
        \caption{Непрерывность винеровского процесса. $\varepsilon = 10^{-5}$.}
\end{figure}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{9_2.png}
        }
        \caption{30 винеровских процессов - все находятся внутри доверительного интервала, вычисленного по правилу трех сигм.}
\end{figure}

\subsection{Процесс Орнштейна--Уленбека}

\begin{definition}
        Случайный процесс $\{W_t\}_{t\in T}$ называется \textit{стационарным}, если конечномерные распределения инвариантны относительно сдвига времени.
\end{definition}

\begin{definition}
        Гауссовский процесс $\{W_t\}_{t\in T}$ называется \textit{процессом Орнштейна--Уленбека}, если он является стационарным и марковским.
\end{definition}

Из стационарности следует, что: $\mathbb{E}W(t) = const = \mu, \, R(t, s) = R(|s-t|)$.

Из того, что процесс марковский: $\rho(s, t) = \rho(s, \tau)\rho(\tau, t)$, где $\rho(s, t)$ - коэффициент корреляции случайных величин $W_s$ и $W_t$.

Пусть $\mathbb{V}ar W(t) = \sigma^2$. Тогда: $R(t, s) = \sigma^2 \rho(s, t)$.

Так как $R(t, s) = R(|s-t|)$, то $\rho(s, t) = \rho(s-t) \Rightarrow \rho(x+y) = \rho(x)\rho(y)$, где $x = s-\tau, \, y = \tau-t$.

\begin{theorem}
        Пусть функция $f(t)$ определена при $t>0$ и ограничена на каждом конечном интервале. Если $f(t)$ удовлетворяет соотношению $f(t+s)=f(t)f(s)$, то либо $f(t) \equiv 0$, либо $f(t) = e^{-\lambda t}$, где $\lambda = const > 0$.
\end{theorem}

Рассмотрим случай, когда $\rho(s, t) = e^{-\lambda |s-t|}$. Пусть $\tilde{x} = (x_1, x_2)^T$.

Так как процесс Орнштейна-Уленбека гауссовский:
$$
\rho_{W(t), W(s)}(\tilde{x}) = \frac{1}{2\pi\sqrt{|R|}} e^{-\frac{1}{2} \tilde{x}^T R^{-1} \tilde{x}},
$$
$$
\rho_{W(s)}(x_2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x_2^2}{2\sigma^2}}.
$$
Ковариационная матрица:
$R = \begin{bmatrix} \sigma^2& \sigma^2 e^{-\lambda |s-t|}\\ \sigma^2 e^{-\lambda |s-t|}& \sigma^2 \end{bmatrix}.$

Тогда:
$$
\rho_{W(t)}(x_1|W(s) = x_2) = \frac{\rho_{W(t), W(s)}((x_1, x_2))}{\rho_{W(s)}(x_2)}= \frac{1}{\sigma\sqrt{2\pi\left(1 - e^{-2\lambda|t-s|}\right)}}exp\left\{-\frac{\left(x_1 - x_2 e^{-\lambda |t-s|}\right)^2}{2\sigma^2\left(1 - e^{-2\lambda|t-s|}\right)}\right\}.
$$
Чтобы смоделировать процесс Орнштейна-Уленбека, будем действовать аналогично тому, как поступали с винеровским процессом. В данном случае будем делить отрезки пополам: $t = \frac{t_2-t_1}{2}$, ковариационные матрицы будут выглядеть следующим образом:

$R_2 = \sigma^2\begin{bmatrix} 1& e^{-\lambda(t_2-t_1)}\\ e^{-\lambda(t_2-t_1)}& 1 \end{bmatrix}, \, R_3 = \sigma^2\begin{bmatrix} 1& e^{-\lambda(t-t_1)}& e^{-\lambda(t_2-t_1)}\\ e^{-\lambda(t-t_1)}& 1& e^{-\lambda(t_2-t_1)}\\ e^{-\lambda(t_2-t_1)}& e^{-\lambda(t_2-t_1)}& 1 \end{bmatrix}$

В конечном итоге получим, что:
$$
W(t) \sim N\left((x_1 + x_2)\frac{e^{-\frac{\lambda(t_2-t_1)}{2}}}{1 + e^{-\lambda(t_2-t_1)}}, \sigma^2\frac{1-e^{-\lambda(t_2-t_1)}}{1+e^{-\lambda(t_2-t_1)}}\right).
$$

Если $\rho(t) \equiv 0$, то $cov(W(t), W(s)) \equiv 0$, поэтому, так как процесс $W(t)$ - гауссовский, $W(t)$ независимы в совокупности. В этом случае моделирование процесса Орнштейна-Уленбека заключается в моделировании случайных величин с распределением $N(a, \sigma^2).$

\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{9_3.png}
        }
        \caption{Непрерывность процесса Орштейна--Уленбека. $\lambda = 25$, $\sigma^2 = 1$, $\varepsilon = 10^{-5}$.}
\end{figure}
\begin{figure}[H]
        \noindent
        \centering
        {
                \includegraphics[width=120mm]{9_4.png}
        }
        \caption{30 процессов Оршетейна--Уленбека - все находятся внутри доверительного интервала, вычисленного по правилу трех сигм. $\lambda = 15$, $\sigma^2 = 4$, $\varepsilon = 10^{-4}$.}
\end{figure}




\section{Задание №10}

Провести фильтрацию одномерного процесса Орнштейна--Уленбека:
\begin{enumerate}
        \item Используя генератор белого шума, добавить случайную ошибку с известной дисперсией к реализации процесса Орнштейна--Уленбека.
        \item При помощи фильтра Калмана оценить траекторию процесса по зашумленному сигналу. Параметры процесса и белого шума считать известными.
        \item Рассмотреть случай, когда шум 
        \begin{itemize}
                \item является гауссовским,
                \item имеет распределение Коши.
        \end{itemize}
\end{enumerate}

\subsection{Фильтр Калмана}
Пусть:

$x_{n+1} = a x_n + \nu_n,$ где $\nu_n \sim N(0, q), \, x_1 \sim N(0, \sigma^2)$. Это исходный процесс, который мы хотим восстановить с помощью фильтра.

$y_n = x_n + \varepsilon_n,$ где $\varepsilon_n \sim N(0, r).$ Это зашумленный процесс, который мы наблюдаем и подаем на вход фильтру.

Зная параметры исходного процесса $\sigma$ и $\lambda$, найдем параметры $a, q$:

$$R(t_n, t_n) = \sigma^2 = \mathbb{V}ar(x_n),$$

$$R(t_n, t_{n+1}) = \sigma^2 e^{-\lambda(t_{n+1}-t_n)} = Cov(x_n, x_{n+1}) = a \mathbb{V}ar(x_n) = a \sigma^2,$$

$$R(t_{n+1}, t_{n+1}) = \sigma^2 = \mathbb{V}ar(x_{n+1}) = a^2 \mathbb{V}ar(x_n) + q = a^2 \sigma^2 + q.$$

Отсюда следует, что: $a = e^{-\lambda(t_{n+1}-t_n)}, \, q = \sigma^2(1 - e^{-2\lambda(t_{n+1}-t_n)})$

Будем восстанавливать значения $x$ по следующему алгоритму:
\begin{itemize}
    \item $x_{0|0} = \mathbb{E}x_0 = 0$
    \item $R_{0|0} = \mathbb{V}ar x_0 = \sigma^2$
    \item $x_{k+1|k} = a x_{k|k}$
    \item $R_{k+1|k} = a^2 R_{k|k} + q$
    \item $x_{k+1|k+1} = x_{k+1|k}\left(1 - \frac{R_{k+1|k}}{R_{k+1|k} + r}\right) + \frac{R_{k+1|k}}{R_{k+1|k} + r}y_{k+1}$
    \item $R_{k+1|k+1} = R_{k+1|k}\left(1 - \frac{R_{k+1|k}}{R_{k+1|k} + r}\right)$
\end{itemize}

Доверительный интервал будет выглядеть следующим образом:
$$
[x-3\sqrt{R}, x+3\sqrt{R}].
$$

\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{10_1.png}
        \caption{Результаты работы фильтра Калмана для гауссовского шума. $\lambda = 12$, $\sigma^2 = 1$.}
\end{figure}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{10_2.png}
        \caption{Результаты работы фильтра Калмана для шума с распределением $C(0, 0.1)$.}
\end{figure}
\begin{figure}[H]
        \centering
        \includegraphics[width=120mm]{10_3.png}
        \caption{Результаты работы фильтра Калмана для шума с распределением $C(0, 0.1)$. Фильтр работает плохо: наблюдаются сильные выбросы в доверительных интервалах из-за тяжелых хвостов распределения Коши. }
\end{figure}
\clearpage



\section{Задание №11}

Построить двумерное пуассоновское поле, отвечающее сложному пуассоновскому процессу:
\begin{enumerate}
        \item Первая интерпретация: система массового обслуживания. При этом первая координата поля --- время поступления заявки в СМО (равномерное распределение), вторая --- время её обслуживания (распределение $\chi^2$ с 10-ю степенями свободы).
        \item Вторая интерпретация: система массового обслуживания с циклической интенсивностью $\lambda (1 + \cos(t))$ и единичными скачками. Свести данную задачу моделирования неоднородного пуассоновского процесса при помощи метода Льюиса и Шедлеaра к моделированию двумерного пуассоновского поля, где первая координата имеет равномерное распределение, а вторая --- распределение Бернулли.
        \item Третья интерпретация: работа страховой компании. Первая координата~--- момент наступления страхового случая (равномерное распределение), вторая координата~--- величина ущерба (распределение Парето). Поступление капитала по времени линейно со скоростью $c > 0$, начальный капитал $W > 0$.
        \item Для каждой системы рассмотреть всевозможные случаи поведения системы в зависимости от значения параметров.
\end{enumerate}


\subsection{Система массового обслуживания}

Рассмотрим систему на отрезке времени $[0, T]$. Будем генерировать $t_i \sim U[0, T]$ - времена поступления заявок, $s_i \sim \chi^2(10)$ - времена обработки заявок.

Тогда время окончания обработки $i$-ой заявки:
$$
Q_i = 
\begin{equation*}
 \begin{cases}
   t_i + s_i,&\text{если обработка $(i-1)$ - ой заявки в момент времени $t_i$ уже завершена,} 
   \\
   Q_{i-1} + s_i,&\text{иначе.}
 \end{cases}
\end{equation*}
$$

То есть: $Q_i = t_i + max\{0, \,Q_{i-1} - t_i\} + s_i.$

Будем подсчитывать количество заявок, ожидающих обработки, во время поступления новой заявки:
$$
n_i - \text{количество заявок $j$, таких, что: } 
\begin{equation*}
 \begin{cases}
   j<i, 
   \\
   Q_j > t_i.
 \end{cases}
\end{equation*} 
$$

Так как $\delta_i = (t_{i+1} - t_i) \sim Exp(\lambda)$, $\mathbb{E}\delta_i = \frac{1}{\lambda}$ - среднее время между поступлениями заявок. При этом среднее время обработки заявки: $\mathbb{E}s_i = 10$.

Тогда:
\begin{enumerate}
    \item если $\frac{1}{\lambda} = 10$, то скорость поступления заявок совпадает со скоростью их обработки,
    \item если $\frac{1}{\lambda} > 10$, то скорость обработки заявок превышает скорость их поступления,
    \item если $\frac{1}{\lambda} < 10$, то заявки не успевают обрабатываться и накапливаются в очереди.
\end{enumerate}

\begin{figure}[H]
        \noindent
        \centering
        {
        \includegraphics[width=120mm]{11_005.png}}
        \caption{При $\lambda = 0.05$ очередь почти не образуется.} 
\end{figure}
\begin{figure}[H]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_01.png}}
        \caption{При $\lambda = 0.1$ наблюдаем относительное равновесие.}
\end{figure}
\begin{figure}[H]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_03.png}}
        \caption{При $\lambda =0.3$ очередь неограниченно растет.}
\end{figure}


\subsection{Система массового обслужиания с циклической интенсивностью и единичными скачками}

Будем генерировать $t_i \sim U[0, T]$ - времена поступления заявок, $s_i \sim \chi^2(10)$ - времена обработки заявок.

Введем обозначение:

$$\lambda(t) = \lambda(1 + cos(t)) \leqslant 2\lambda = \lambda^*.$$

Тогда по методу Льюиса и Шедлеaра оставим $t_i$ с вероятностью $\displaystyle\frac{\lambda(t_i)}{\lambda^*}$:

Пусть $\xi \sim U[0, 1]$, тогда: $$ P\left(\xi < \displaystyle\frac{\lambda(t_i)}{\lambda^*}\right) = F_{\xi}\left(\displaystyle\frac{\lambda(t_i)}{\lambda^*}\right) = \displaystyle\frac{\lambda(t_i)}{\lambda^*}.$$ Поэтому будем оставлять только те $t_i$, для которых выполнено: $\xi_i < \displaystyle\frac{\lambda(t_i)}{\lambda^*}.$

Поведение системы системы в зависимости от значений параметра $\lambda$ остается тем же, что и в обычной системе массового обслуживания, так как средняя мгновенная интенсивность сохраняется: она равна $\lambda$.

\begin{figure}[H]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_1_005.png}}
        \caption{При $\lambda = 0.05$ очередь почти не образуется.}
\end{figure}

\begin{figure}[H]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_1_01.png}}
        \caption{При $\lambda = 0.1$ наблюдаем относительное равновесие.}
\end{figure}

\begin{figure}[H]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_1_08.png}}
        \caption{При $\lambda = 0.8$ очередь неограниченно растет.}
\end{figure}

\subsection{Работа страховой компании}

\begin{definition}
        Случайная величина $\xi$ имеет \textit{распределение Парето} с параметрами $x_m$ и $k$, если ее функция распределения имеет вид
$$
        F_{\xi}(x) = 1 - \left(\frac{x_m}{x}\right)^k.
$$
\end{definition}

Для моделирования паретовской случайной величины воспользуемся методом обращения функции распределения. Обратная функция имеет вид:
$$
        F_{\xi}^{-1}(x)
        =
        \frac{x_m}{1-x}^{\nicefrac{1}{k}}.
$$

Будем генерировать $t_i \sim Pois(\lambda), \, s_i \sim P(x_m, k)$, где $s_i$ - величина ущерба.

Тогда величина капитала страховой компании: $$W(t) = W(0) + ct - s(t),\text{ где } s(t) = \sum\limits_{t_i<t}s_i.$$

Страховая компания разорится в момент времени: $T = \min\{t>0 \,|\, W(t) < 0\}$.

Из того, что $(t_{i+1}-t_i) \sim Exp(\lambda), \, \mathbb{E}s_i = \displaystyle\frac{k x_m}{k-1}$, получим:

$$\mathbb{E}W'(t) = c - \mathbb{E}'s(t) = c - \left(\sum\limits_{t_i<t} \mathbb{E}s_i\right)' = c - \left(t\lambda \frac{k x_m}{k-1}\right)' = c - \lambda \frac{k x_m}{k-1}.$$

Откуда следует, что:
\begin{enumerate}
    \item При $c(k-1) = \lambda k x_m$ капитал страховой системы не изменяется,
    \item При $c(k-1) < \lambda k x_m$ капитал будет уменьшаться,
    \item При $c(k-1) > \lambda k x_m$ капитал будет увеличиваться.
\end{enumerate}

\begin{figure}[h]
        \noindent
        \centering
        {
        \includegraphics[width=120mm]{11_2_01.png}}
        \caption{Увеличение капитала при $c = 1, \, k = 2, \, x_m = 1, \, \lambda = 0.1$.}
\end{figure}
\clearpage
\begin{figure}[t]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_2_05.png}}
        \caption{Относительное равновесие при тех же параметрах, кроме $\lambda = 0.5$.}
\end{figure}
\begin{figure}[b]
\noindent
        \centering
        {
        \includegraphics[width=120mm]{11_2_1.png}}
        \caption{Уменьшение капитала при тех же параметрах, кроме $\lambda = 1$.}
\end{figure}



\clearpage
\begin{thebibliography}{9}
    \bibitem{shiryaev}Ширяев А. Н. Вероятность, в 2-х кн. --- 4-е изд., переработанное и дополненное --- М.: МЦНМО, 2007.
    \bibitem{a} Феллер В. Введение в теорию вероятностей и её приложения. --- М.: Мир, 1964.
    \bibitem{b} Кропачёва Н.Ю., Тихомиров А.С. Моделирование случайных величин: Метод. указания. --- НовГУ им. Ярослава Мудрого --- Великий Новгород, 2004.
    \bibitem{novak} Novak S.Y. Extreme value methods with applications to finance. London: CRC/ Chapman and Hall/Taylor and Francis, 2011.
    \bibitem{c} Булинский А. В., Ширяев А. Н. Теория случайных процессов. --- М.: «ФИЗМАТЛИТ», 2005.
    \bibitem{d} Лопаткин А. С. Метод отжига --- Санкт-Петербургский государственный университет, 2005.
    \bibitem{e} Соболь И.М. Численные методы Монте-Карло. --- Наука, 1973
    \bibitem{f} Heinonen J. Lectures on Lipschitz analysis. Department of mathematics, University of Michigan, Ann Arbor, USA.
\end{thebibliography}
\end{document}